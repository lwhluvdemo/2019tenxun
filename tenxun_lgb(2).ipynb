{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\nfrom collections import Counter\nfrom sklearn.feature_selection import mutual_info_classif\nwarnings.filterwarnings('ignore')\nimport time,datetime\nimport gc\n\ntotal_Exposure_Log_path = 'https://mltest8535517255.blob.core.windows.net/tenxun/test_a/totalExposureLog.csv?sp=r&st=2019-04-30T16:01:43Z&se=2019-05-16T00:01:43Z&spr=https&sv=2018-03-28&sig=heGUf135IcugHV11B7ZsY2%2BwwHipMbryyV7EVDzPTL8%3D&sr=b'\nuser_data_path = 'https://mltest8535517255.blob.core.windows.net/tenxun/test_a/user_data.csv?sp=r&st=2019-04-30T16:01:13Z&se=2019-05-16T00:01:13Z&spr=https&sv=2018-03-28&sig=0u50xAwvE4Xh9%2FfvkMnLDKDDuZxU1PuHYzSHGExSZk8%3D&sr=b'\nad_operation_path = 'https://mltest8535517255.blob.core.windows.net/tenxun/test_a/ad_operation.csv?sp=r&st=2019-04-30T15:56:43Z&se=2019-05-15T23:56:43Z&spr=https&sv=2018-03-28&sig=%2F58qmlj%2BRXSncTcMqI%2Bk9iIgIjWTy29Xa1PRXzkVUcE%3D&sr=b'\nad_static_feature_path = 'https://mltest8535517255.blob.core.windows.net/tenxun/test_a/ad_static_feature.csv?sp=r&st=2019-04-30T15:57:55Z&se=2019-05-15T23:57:55Z&spr=https&sv=2018-03-28&sig=LOYvgwdPxCbfljGqJSoWpKS0JK0w%2BTg5nhv26v1me7U%3D&sr=b'\ntest_sample_path = 'https://mltest8535517255.blob.core.windows.net/tenxun/test_a/test_sample.csv?sp=r&st=2019-04-30T16:00:28Z&se=2019-05-16T00:00:28Z&spr=https&sv=2018-03-28&sig=1jmDaOQ5zxyOGLqdE42GF7EBUY6qxuNjTRe%2F57tN8v4%3D&sr=b'\n\ntotal_Exposure_Log = pd.read_csv(total_Exposure_Log_path,sep='\\t',usecols = [1,4,6],header = None)\nnames = ['ad_request_time','ad_id','bid']\ntotal_Exposure_Log.columns = names\n# names=['ad_request_id','ad_request_time','ad_space_id','user_id','ad_id',\n#                                                                          'ad_size','bid','pctr',' quality_ecpm','total_Ecpm']\n\n\n# # user_data = pd.read_csv(user_data_path,sep='\\t',names=['user_id','age','gender','area','status','edu','ConAbility',\n# #                                                        'device','work','CType','behhavior'])\nad_operation = pd.read_csv(ad_operation_path,sep='\\t',names=['ad_id','Change_time','Operation_type',\n                                                             'Modify_field','value_after_operation'])\nad_static_feature = pd.read_csv(ad_static_feature_path,sep='\\t',names=['ad_id','Creation_time','ad_account_id','Product_id',\n                                                                       'Product_Types','ad_industry_id','Material_size'])\ntest_sample = pd.read_csv(test_sample_path,sep='\\t',names=['Sample_id','ad_id','Creation_time','Material_size','ad_industry_id','Product_Types',\n                                                           'Product_id','ad_account_id','Delivery_period','Crowd_orientation','ad_bid'])\ntotal_Exposure_Log['ad_request_time'] = pd.to_datetime(total_Exposure_Log['ad_request_time'],unit='s')\ntotal_Exposure_Log.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"total_Exposure_Log['month'] = total_Exposure_Log['ad_request_time'].dt.month\ntotal_Exposure_Log['day'] = total_Exposure_Log['ad_request_time'].dt.day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = total_Exposure_Log.groupby(['ad_id','month','day']).size().reset_index(name='target')\ny_train = y_train.drop(['month','day'],axis=1)\ntotal_Exposure_Log = total_Exposure_Log.drop(['month','day'],axis = 1)\nad_bid = total_Exposure_Log['bid'].groupby([total_Exposure_Log['ad_id']]).mean().reset_index(name='ad_bid')\ntotal_Exposure_Log = total_Exposure_Log.drop(['bid'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = y_train.merge(ad_bid, on='ad_id', how='left')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.rename(columns = {'ad_size':'Material_size'},inplace = True)\n# train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del total_Exposure_Log; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ad_operation = ad_operation.replace({'Change_time':0},np.nan)\nad_static_feature = ad_static_feature.replace({'Product_id':'-1'},np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sample['Creation_time'] = pd.to_datetime(test_sample['Creation_time'],unit='s')\nad_operation['Change_time'] = pd.to_datetime(ad_operation.Change_time, format='%Y%m%d%H%M%S', errors='coerce')\nad_static_feature['Creation_time'] = pd.to_datetime(ad_static_feature['Creation_time'],unit='s')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ad_static_feature = (ad_static_feature.set_index(['ad_id','Creation_time','ad_account_id','Product_id',\n                                                  'Product_Types','ad_industry_id']).astype(str)\n   .stack()\n   .str.split(',', expand=True)\n   .stack()\n   .unstack(-2)\n   .reset_index(-1, drop=True)\n   .reset_index()\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ad_static_feature['Product_id'] = pd.to_numeric(ad_static_feature['Product_id'], errors='coerce')\nad_static_feature['ad_industry_id'] = pd.to_numeric(ad_static_feature['ad_industry_id'], errors='coerce')\nad_static_feature['Material_size'] = pd.to_numeric(ad_static_feature['Material_size'], errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ad_static_feature.Material_size = ad_static_feature.Material_size.fillna(ad_static_feature.Material_size.median())\nad_static_feature.Material_size = ad_static_feature.Material_size.astype(int)\n\nad_static_feature.ad_industry_id = ad_static_feature.ad_industry_id.fillna(method='pad')\nad_static_feature.ad_industry_id = ad_static_feature.ad_industry_id.astype(int)\n\n# 商品 id： 广告推广目标的唯一标识， 若推广目标是落地页，则该字段为空（应该是有意义的）\n# ad_static_feature.Product_id = ad_static_feature.Product_id.fillna(0)\n# ad_static_feature.Product_id = ad_static_feature.Product_id.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Resource_Features = ['ad_account_id','Product_id','Product_Types','ad_industry_id','Material_size','ad_bid']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(ad_static_feature,on = 'ad_id',how = 'left')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\n# Nullity or missing values by columns\nmsno.matrix(df=train.iloc[:,3:11], figsize=(20, 14), color=(0.42, 0.1, 0.05))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.target = train.target.fillna(train.target.median())\n# train.target = train.target.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sample = test_sample.sort_values(by = ['ad_id'])\ntest_sample.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sample.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To check how many columns have missing values - this can be repeated to see the progress made\ndef show_missing():\n    missing = train.columns[train.isnull().any()].tolist()\n    return missing\n\n# Looking at categorical values\ndef cat_exploration(column):\n    return train[column].value_counts()\n\n# Imputing the missing values\ndef cat_imputation(column, value):\n    train.loc[train[column].isnull(),column] = value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cat_exploration('target')\nimport seaborn as sns\n%pylab inline\nsns.pairplot(train[['target','ad_bid','Material_size']].dropna())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout, BatchNormalization\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers.noise import GaussianDropout\nfrom keras.optimizers import Adam\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Imputer\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 三"},{"metadata":{"trusted":true},"cell_type":"code","source":"def gini(y, pred):\n    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n    gs -= (len(y) + 1) / 2.\n    return gs / len(y)\n\ndef gini_lgb(preds, dtrain):\n    y = list(dtrain.get_label())\n    score = gini(y, preds) / gini(y, y)\n    return 'gini', score, True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train.drop(['target','Creation_time'], axis=1)\nX = train_df.values\n\nfeatures = train_df.columns\n\ntarget = train['target']\ny = train['target'].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"nrounds=1000  # need to change to 2000\nkfold = 5\n\nparams = {'max_bin':20, 'learning_rate' : 0.002, 'boosting_type':'gbdt',  'objective': 'regression', 'metric': 'l1', 'max_depth':10,\n          'feature_fraction': 0.8, 'bagging_fraction':0.9, 'bagging_freq':20, 'min_data': 1000,'num_leaves':512}\n\nfolds = StratifiedKFold(n_splits=kfold, shuffle=False, random_state=1)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test_sample))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    clf = lgb.train(params, trn_data, nrounds, valid_sets = [trn_data, val_data],feval=gini_lgb, verbose_eval=100, early_stopping_rounds = 300)\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test_sample[features], num_iteration=clf.best_iteration) / folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(r2_score(target, oof)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.save_model('model.txt')\n# clf = lgb.Booster(model_file='model.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint(\"CV score: {:<8.5f}\".format(r2_score(target, oof)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.DataFrame({\"ID_code\":test_sample[\"Sample_id\"].values})\nsub_df[\"target\"] = predictions\n# sub_df = sub_df.sort_values(by = ['ID_code'])\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"submission_sub_df.csv\", index=False,float_format='%.4f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M_train = train.drop(['ad_account_id','Product_id','Product_Types','ad_industry_id',\n                    'Material_size','Creation_time','target'],axis = 1) \nMonotonic_sub=test_sample['Sample_id'].to_frame()\nMonotonic_sub['target'] = sub_df['target']+M_train['ad_bid']/10000\n# Monotonic_sub = Monotonic_sub.sort_values(by = [''])\nMonotonic_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Monotonic_sub.to_csv('submission_mon.csv', index=False, float_format='%.4f') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 尝试利用规则"},{"metadata":{},"cell_type":"markdown","source":"* **新广告用模型结果填充**\n* **旧广告用历史平均**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rule_train = train['target'].groupby(train['ad_id']).mean().reset_index(name='ad_mean_target')\nrule_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df_Monotonic = test_sample.drop(['Creation_time','Material_size','ad_industry_id','Product_Types',\n                                     'Product_id','ad_account_id','Delivery_period','Crowd_orientation','ad_bid'],axis = 1)\nsub_df_Monotonic.sort_values(by = ['ad_id'])\nsub_df_Monotonic.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sub_df_Monotonic = test_sample.drop(['Creation_time','Material_size','ad_industry_id','Product_Types',\n#                                      'Product_id','ad_account_id','Delivery_period','Crowd_orientation','ad_bid'],axis = 1)\n# sub_df_Monotonic.sort_values(by = ['ad_id'])\n# sub_df_Monotonic = pd.DataFrame({\"ID_code\":test_sample[\"Sample_id\"].values})\ndef rule_filling():\n    if sub_df_Monotonic.ad_id.isin(rule_train.ad_id)==True:\n        return sub_df_Monotonic[\"target\"] = rule_train.loc[rule_train.ad_id]ad_mean_target.values\n\nsub_df_Monotonic[\"target\"].apply(lambda x : predictions if x ) = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df_Monotonic.to_csv('submission_sub_df_mon.csv', index=False, float_format='%.4f') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Second**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# custom objective function (similar to auc)\n\ndef gini(y, pred):\n    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n    gs -= (len(y) + 1) / 2.\n    return gs / len(y)\n\ndef gini_xgb(pred, y):\n    y = y.get_label()\n    return 'gini', gini(y, pred) / gini(y, y)\n\ndef gini_lgb(preds, dtrain):\n    y = list(dtrain.get_label())\n    score = gini(y, preds) / gini(y, y)\n    return 'gini', score, True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train.drop(['target','Creation_time'], axis=1)\nX = train_df.values\n\nfeatures = train_df.columns\n\ntarget = train['target']\ny = train['target'].values\n\nsub=test_sample['ad_id'].to_frame()\nsub['target']=0\ncategory_features = ['Product_Types','ad_industry_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrounds=1000  # need to change to 2000\nkfold = 5\n# lgb\nparams = {'max_bin':20, 'learning_rate' : 0.002, 'boosting_type':'gbdt',  'objective': 'regression', 'metric': 'l1', 'max_depth':10,\n          'feature_fraction': 0.8, 'bagging_fraction':0.9, 'bagging_freq':20, 'min_data': 1000,'num_leaves':512}\n\nskf = StratifiedKFold(n_splits=kfold, random_state=1)\nfor i, (train_index, test_index) in enumerate(skf.split(X, y)):\n    print(' lgb kfold: {}  of  {} : '.format(i+1, kfold))\n    X_train, X_eval = X[train_index], X[test_index]\n    y_train, y_eval = y[train_index], y[test_index]\n    lgb_model = lgb.train(params, lgb.Dataset(X_train, label=y_train), \n                          nrounds, lgb.Dataset(X_eval, label=y_eval), verbose_eval=100, \n                          feval=gini_lgb, early_stopping_rounds=300)\n    sub['target'] += lgb_model.predict(test_sample[features].values,num_iteration=lgb_model.best_iteration) / (2*kfold)\n    p_test = clf.predict(test_sample[features])\n# ,feature_name=features, categorical_feature=category_features\n\n# gc.collect()\nsub.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['ad_account_id','Product_id','Product_Types','ad_industry_id',\n                    'Material_size','Creation_time','target'],axis = 1) \nMonotonic_sub=test_sample['Sample_id'].to_frame()\nMonotonic_sub['target'] = sub['target']+train['ad_bid']/10000\nMonotonic_sub = Monotonic_sub.sort_values(by = ['ad_id'])\nMonotonic_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Monotonic_sub.to_csv('submission_mon.csv', index=False, float_format='%.4f') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sub.sort_values(by = ['ad_id'])\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission_sub.csv', index=False, float_format='%.4f') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **First**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = df_train[Resource_Features]\nX = x_train.values\ny_train = df_train['target'].values\nx_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.astype(np.float32, copy=False)\nd_train = lgb.Dataset(x_train, label=y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {}\nparams['max_bin'] = 20\nparams['learning_rate'] = 0.0021 # shrinkage_rate\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'l1'          # or 'mae'\nparams['sub_feature'] = 0.345    # feature_fraction (small values => use very different submodels)\nparams['bagging_fraction'] = 0.85 # sub_row\nparams['bagging_freq'] = 40\nparams['num_leaves'] = 512        # num_leaf\nparams['min_data'] = 500         # min_data_in_leaf\nparams['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\nparams['verbose'] = 0\nparams['feature_fraction_seed'] = 2\nparams['bagging_seed'] = 3\n\nnp.random.seed(0)\nrandom.seed(0)\n\nprint(\"\\nFitting LightGBM model ...\")\nclf = lgb.train(params, d_train, 430)\n\ndel d_train; gc.collect()\ndel x_train; gc.collect()\n\nprint(\"\\nPrepare for LightGBM prediction ...\")\n#*****\n\n# print(\"   Read sample file ...\")\n# sample = pd.read_csv('../input/sample_submission.csv')\n# print(\"   ...\")\n# sample['parcelid'] = sample['ParcelId']\n# print(\"   Merge with property data ...\")\n# df_test = sample.merge(prop, on='parcelid', how='left')\n# print(\"   ...\")\n# del sample, prop; gc.collect()\n# print(\"   ...\")\n\ntest_sample = pd.read_csv(test_sample_path,sep='\\t',names=['Sample_id','ad_id','Creation_time','Material_size','ad_industry_id','Product_Types',\n                                                           'Product_id','ad_account_id','Delivery_period','Crowd_orientation','ad_bid'])\n\n\n#df_test['Ratio_1'] = df_test['taxvaluedollarcnt']/df_test['taxamount']\nx_test = test_sample[Resource_Features]\nprint(\"   ...\")\ndel test_sample; gc.collect()\nprint(\"   Preparing x_test...\")\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\nprint(\"   ...\")\nx_test = x_test.values.astype(np.float32, copy=False)\n\nprint(\"\\nStart LightGBM prediction ...\")\np_test = clf.predict(x_test)\n\ndel x_test; gc.collect()\n\nprint( \"\\nUnadjusted LightGBM predictions:\" )\nprint( pd.DataFrame(p_test).head() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sample = pd.read_csv(test_sample_path,sep='\\t',names=['Sample_id','ad_id','Creation_time','Material_size','ad_industry_id','Product_Types',\n                                                           'Product_id','ad_account_id','Delivery_period','Crowd_orientation','bid'])\n\n\nsub_df = pd.DataFrame({\"ID_code\":test_sample[\"Sample_id\"].values})\nsub_df['ID_code'] = pd.to_numeric(sub_df['ID_code'], errors='coerce')\nsub_df['ID_code'] = [float(format(x, '.4f')) for x in sub_df]\nsub_df[\"target\"] = p_test*0.91\nsub_df.to_csv(\"submission.csv\", index=False,header = None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}